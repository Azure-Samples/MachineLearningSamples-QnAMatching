{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Phrase Learning\n",
    "\n",
    "If you haven't complete the **Part 1: Data Preparation**, please complete it before moving forward with **Part 2: Phrase Learning**. Part 2 requires files created from Part 1.\n",
    "\n",
    "**NOTE**: Python 3 kernel doesn't include Azure Machine Learning Workbench functionalities. Please switch the kernel to `local` before continuing further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Python Modules\n",
    "\n",
    "`modules.phrase_learning` contains a list of Python user-defined Python modules to learn informative phrases that are used in this examples. You can find the source code of those modules in the directory of `modules/phrase_learning.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os, requests, warnings\n",
    "from collections import (namedtuple, Counter)\n",
    "from modules.phrase_learning import (CleanAndSplitText, ComputeNgramStats, RankNgrams, ApplyPhraseRewrites,\n",
    "                            ApplyPhraseLearning, ApplyPhraseRewritesInPlace, ReconstituteDocsFromChunks,\n",
    "                            CreateVocabForTopicModeling)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access trainQ and testQ from Part 1\n",
    "\n",
    "As we have prepared the _trainQ_ and _testQ_ from the `Part 1: Data Preparation`, we retrieve the datasets here for the further process.\n",
    "\n",
    "_trainQ_ contains 5,153 training examples and _testQ_ contains 1,735 test examples. Also, there are 103 unique answer classes in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load non-content bearing function words (.txt file) into a Python dictionary. \n",
    "def LoadListAsHash(fileURL):\n",
    "    response = requests.get(fileURL, stream=True)\n",
    "    wordsList = response.text.split('\\n')\n",
    "\n",
    "    # Read in lines one by one and strip away extra spaces, \n",
    "    # leading spaces, and trailing spaces and inserting each\n",
    "    # cleaned up line into a hash table.\n",
    "    listHash = {}\n",
    "    re1 = re.compile(' +')\n",
    "    re2 = re.compile('^ +| +$')\n",
    "    for stringIn in wordsList:\n",
    "        term = re2.sub(\"\",re1.sub(\" \",stringIn.strip('\\n')))\n",
    "        if term != '':\n",
    "            listHash[term] = 1\n",
    "    return listHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workfolder = os.environ.get('AZUREML_NATIVE_SHARE_DIRECTORY')\n",
    "\n",
    "# paths to trainQ, testQ and function words.\n",
    "trainQ_path = os.path.join(workfolder, 'trainQ_part1')\n",
    "testQ_path = os.path.join(workfolder, 'testQ_part1')\n",
    "function_words_url = 'https://bostondata.blob.core.windows.net/stackoverflow/function_words.txt'\n",
    "\n",
    "# load the training and test data.\n",
    "trainQ = pd.read_csv(trainQ_path, sep='\\t', index_col='Id', encoding='latin1')\n",
    "testQ = pd.read_csv(testQ_path, sep='\\t', index_col='Id', encoding='latin1')\n",
    "\n",
    "# Load the list of non-content bearing function words.\n",
    "functionwordHash = LoadListAsHash(function_words_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Split the Text\n",
    "\n",
    "The CleanAndSplitText function from __phrase_learning__ takes as input a list where each row element is a single cohesive long string of text, i.e. a \"question\". The function first splits each string by various forms of punctuation into chunks of text that are likely sentences, phrases or sub-phrases. The splitting is designed to prohibit the phrase learning process from using cross-sentence or cross-phrase word strings when learning phrases.\n",
    "\n",
    "The function returns a table where each row represents a chunk of text from the questions. The `DocID` coulmn indicates the original row index from associated question in the input from which the chunk of text originated. The `DocLine` column contains the original text excluding the punctuation marks and `HTML` markup that have been during the cleaning process. The `Lowercase Taxt` column contains a fully lower-cased version of the text in the `CleanedText` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "CleanedTrainQ = CleanAndSplitText(trainQ)\n",
    "CleanedTestQ = CleanAndSplitText(testQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>LowercaseText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69913</td>\n",
       "      <td>0</td>\n",
       "      <td>why don't self-closing script tags work</td>\n",
       "      <td>why don't self-closing script tags work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69913</td>\n",
       "      <td>1</td>\n",
       "      <td>what is the reason browsers do not correctly r...</td>\n",
       "      <td>what is the reason browsers do not correctly r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69913</td>\n",
       "      <td>2</td>\n",
       "      <td>only this is recognized</td>\n",
       "      <td>only this is recognized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69913</td>\n",
       "      <td>3</td>\n",
       "      <td>does this break the concept of xhtml support</td>\n",
       "      <td>does this break the concept of xhtml support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69913</td>\n",
       "      <td>4</td>\n",
       "      <td>note</td>\n",
       "      <td>note</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DocID  DocLine                                        CleanedText  \\\n",
       "0  69913        0            why don't self-closing script tags work   \n",
       "1  69913        1  what is the reason browsers do not correctly r...   \n",
       "2  69913        2                            only this is recognized   \n",
       "3  69913        3       does this break the concept of xhtml support   \n",
       "4  69913        4                                               note   \n",
       "\n",
       "                                       LowercaseText  \n",
       "0            why don't self-closing script tags work  \n",
       "1  what is the reason browsers do not correctly r...  \n",
       "2                            only this is recognized  \n",
       "3       does this break the concept of xhtml support  \n",
       "4                                               note  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CleanedTrainQ.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Informative Phrases \n",
    "The phrases can be treated as single compound word units in down-stream processes such as discriminative training. To learn the phrases, we have implemented the basic framework for key phrase learning as described in the paper entitled [\"Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech\"](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf) which was originally presented in the 2012 IEEE Workshop on Spoken Language Technology. Although the paper examines the use of the technology for analyzing human-to-human conversations, the techniques are quite general and can be applied to a wide range of natural language data including news stories, legal documents, research publications, social media forum discussions, customer feedback forms, product reviews, and many more.\n",
    "\n",
    "`ApplyPhraseLearning` module takes the following arguments:\n",
    "- `textData`: a list of text data.\n",
    "- `learnedPhrases`: a list of learned phrases. For initialization, an empty list should be given.\n",
    "- `maxNumPhrases`: maximium number of phrases to learn. If you want to test the code out quickly then set this to a small value (e.g. 100) and set verbose to true when running the quick test.\n",
    "- `maxPhraseLength`: maximum number of words allowed in the learned phrases.\n",
    "- `maxPhrasesPerIter`: maximum number of phrases to learn per iteration. Increasing this number may speed up processing but will affect the ordering of the phrases learned and good phrases could be by-passed if the maxNumPhrases is set to a small number.\n",
    "- `minCount`: minimum number of times a phrase must occur in the data to be considered during the phrase learning process.\n",
    "- `functionwordHash`: a precreated hash table containing the list of function words used during phrase learning.\n",
    "- `blacklistHash`: a precreated hash table (default value: {}) containing the list of black list words to be ignored during phrase learning.\n",
    "- `verbose`: if true, it prints out the learned phrases to stdout buffer while its learning (default value: false). This will generate a lot of text to stdout, so best to turn this off except for testing and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start phrase learning with 0 phrases of 200 phrases learned\n",
      "Iteration 1: Added 42 new phrases in 1.26 seconds (Learned 42 of max 200)\n",
      "Iteration 2: Added 35 new phrases in 1.29 seconds (Learned 77 of max 200)\n",
      "Iteration 3: Added 32 new phrases in 1.24 seconds (Learned 109 of max 200)\n",
      "Iteration 4: Added 34 new phrases in 1.21 seconds (Learned 143 of max 200)\n",
      "Iteration 5: Added 31 new phrases in 1.17 seconds (Learned 174 of max 200)\n",
      "Iteration 6: Added 11 new phrases in 1.19 seconds (Learned 185 of max 200)\n",
      "Iteration 7: Added 3 new phrases in 1.09 seconds (Learned 188 of max 200)\n",
      "Iteration 8: Added 4 new phrases in 1.09 seconds (Learned 192 of max 200)\n",
      "Iteration 9: Added 1 new phrases in 1.08 seconds (Learned 193 of max 200)\n",
      "Iteration 10: Added 1 new phrases in 1.10 seconds (Learned 194 of max 200)\n",
      "Iteration 11: Added 1 new phrases in 0.93 seconds (Learned 195 of max 200)\n",
      "Iteration 12: Added 1 new phrases in 1.04 seconds (Learned 196 of max 200)\n",
      "Iteration 13: Added 1 new phrases in 1.07 seconds (Learned 197 of max 200)\n",
      "Iteration 14: Added 1 new phrases in 1.17 seconds (Learned 198 of max 200)\n",
      "Iteration 15: Added 1 new phrases in 1.16 seconds (Learned 199 of max 200)\n",
      "Iteration 16: Added 1 new phrases in 1.13 seconds (Learned 200 of max 200)\n",
      "*** Phrase learning completed in 0.01 hours ***\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list of learned phrases\n",
    "# If you have completed a partial run of phrase learning\n",
    "# and want to add more phrases, you can use the pre-learned \n",
    "# phrases as a starting point instead and the new phrases\n",
    "# will be appended to the list\n",
    "learnedPhrasesQ = []\n",
    "\n",
    "# Create a copy of the original text data that will be used during learning\n",
    "# The copy is needed because the algorithm does in-place replacement of learned\n",
    "# phrases directly on the text data structure it is provided\n",
    "phraseTextDataQ = []\n",
    "for textLine in CleanedTrainQ['LowercaseText']:\n",
    "    phraseTextDataQ.append(' ' + textLine + ' ')\n",
    "\n",
    "# Run the phrase learning algorithm.\n",
    "ApplyPhraseLearning(phraseTextDataQ, learnedPhrasesQ, maxNumPhrases=200, maxPhraseLength=7, maxPhrasesPerIter=50,\n",
    "                    minCount=5, functionwordHash=functionwordHash)\n",
    "\n",
    "# Add text with learned phrases back into data frame\n",
    "CleanedTrainQ['TextWithPhrases'] = phraseTextDataQ\n",
    "\n",
    "# Apply the phrase learning to test data.\n",
    "CleanedTestQ['TextWithPhrases'] = ApplyPhraseRewritesInPlace(CleanedTestQ, 'LowercaseText', learnedPhrasesQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are some phrases we learned in this part of the tutorial: \n",
      "\n",
      "['possible duplicate', \"i'm trying\", 'works fine', 'doing wrong', 'click event', 'following code', 'using jquery', 'uncaught typeerror', 'ajax request', 'global variable', 'div class', 'json object', 'callback function', \"i'm not sure\", 'anonymous function', 'php file', 'return value', 'user clicks', 'dynamically created', 'input type']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nHere are some phrases we learned in this part of the tutorial: \\n\")\n",
    "print(learnedPhrasesQ[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct the Full Processed Text\n",
    "\n",
    "After replacing the text with learned phrases, we reconstruct the sentences from the chunks of text and insert the sentences in the `TextWithPhrases` field.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstitue the text from seperated chunks.\n",
    "trainQ['TextWithPhrases'] = ReconstituteDocsFromChunks(CleanedTrainQ, 'DocID', 'TextWithPhrases')\n",
    "testQ['TextWithPhrases'] = ReconstituteDocsFromChunks(CleanedTestQ, 'DocID', 'TextWithPhrases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text with Learned Phrases\n",
    "\n",
    "We learn a vocabulary by considering some text exclusion criteria, such as stop words, non-alphabetic words, the words below word count threshold, etc. \n",
    "\n",
    "`TokenizeText` module breaks the reconstituted text into individual tokens and excludes any word that doesn't exist in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TokenizeText(textData, vocabHash):\n",
    "    tokenizedText = ''\n",
    "    for token in textData.split():\n",
    "        if token in vocabHash:\n",
    "            tokenizedText += (token.strip() + ',')\n",
    "    return tokenizedText.strip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words\n",
      "Building vocab\n",
      "Excluded 307 stop words\n",
      "Excluded 911 non-alphabetic words\n",
      "Excluded 15266 words below word count threshold\n",
      "Excluded 142 words below doc count threshold\n",
      "Excluded 3 words above max doc frequency\n",
      "Final Vocab Size: 3114 words\n"
     ]
    }
   ],
   "source": [
    "# create the vocabulary.\n",
    "vocabHashQ = CreateVocabForTopicModeling(trainQ['TextWithPhrases'], functionwordHash)\n",
    "\n",
    "# tokenize the text.\n",
    "trainQ['Tokens'] = trainQ['TextWithPhrases'].apply(lambda x: TokenizeText(x, vocabHashQ))\n",
    "testQ['Tokens'] = testQ['TextWithPhrases'].apply(lambda x: TokenizeText(x, vocabHashQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnswerId</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69913</th>\n",
       "      <td>69984</td>\n",
       "      <td>self-closing,script,tags,work,reason,browsers,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392561</th>\n",
       "      <td>69984</td>\n",
       "      <td>firefox,script,tag,error,adding,basic,script,t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297308</th>\n",
       "      <td>69984</td>\n",
       "      <td>weird,javascript/jquery,behavior,possible_dupl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352182</th>\n",
       "      <td>69984</td>\n",
       "      <td>html,script,tags,ending,possible_duplicate,t,s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5355867</th>\n",
       "      <td>69984</td>\n",
       "      <td>loading,scripts,possible_duplicate,don&amp;#39,t,s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AnswerId                                             Tokens\n",
       "Id                                                                  \n",
       "69913       69984  self-closing,script,tags,work,reason,browsers,...\n",
       "392561      69984  firefox,script,tag,error,adding,basic,script,t...\n",
       "1297308     69984  weird,javascript/jquery,behavior,possible_dupl...\n",
       "3352182     69984  html,script,tags,ending,possible_duplicate,t,s...\n",
       "5355867     69984  loading,scripts,possible_duplicate,don&#39,t,s..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainQ[['AnswerId', 'Tokens']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Outputs to a Share Directory in the Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainQ.to_csv(os.path.join(workfolder, 'trainQ_part2'), sep='\\t', header=True, index=True, index_label='Id')\n",
    "testQ.to_csv(os.path.join(workfolder, 'testQ_part2'), sep='\\t', header=True, index=True, index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qna-matching local",
   "language": "python",
   "name": "qna-matching_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
