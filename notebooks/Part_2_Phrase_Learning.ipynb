{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Phrase Learning\n",
    "\n",
    "If you haven't complete the **Part 1: Data Preparation**, please complete it before moving forward with **Part 2: Phrase Learning**.\n",
    "\n",
    "**NOTE**: Python 3 kernel doesn't include Azure Machine Learning Workbench functionalities. Please switch the kernel to `local` before continuing further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Python Modules\n",
    "\n",
    "`modules.phrase_learning` contains a list of Python user-defined Python modules to learn informative phrases that are used in this examples. You can find the source code of those modules in the directory of `modules/phrase_learning.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os, requests, warnings\n",
    "from collections import (namedtuple, Counter)\n",
    "from modules.phrase_learning import (CleanAndSplitText, ComputeNgramStats, RankNgrams, ApplyPhraseRewrites,\n",
    "                            ApplyPhraseLearning, ApplyPhraseRewritesInPlace, ReconstituteDocsFromChunks,\n",
    "                            CreateVocabForTopicModeling)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access trainQ and testQ from Part 1\n",
    "\n",
    "As we have prepared the _trainQ_ and _testQ_ from the `Part 1: Data Preparation`, we retrieve the datasets here for the further process.\n",
    "\n",
    "_trainQ_ contains 5,153 training examples and _testQ_ contains 1,735 test examples. Also, there are 103 unique answer classes in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load non-content bearing function words (.txt file) into a Python dictionary. \n",
    "def LoadListAsHash(fileURL):\n",
    "    response = requests.get(fileURL, stream=True)\n",
    "    wordsList = response.text.split('\\n')\n",
    "\n",
    "    # Read in lines one by one and strip away extra spaces, \n",
    "    # leading spaces, and trailing spaces and inserting each\n",
    "    # cleaned up line into a hash table.\n",
    "    listHash = {}\n",
    "    re1 = re.compile(' +')\n",
    "    re2 = re.compile('^ +| +$')\n",
    "    for stringIn in wordsList:\n",
    "        term = re2.sub(\"\",re1.sub(\" \",stringIn.strip('\\n')))\n",
    "        if term != '':\n",
    "            listHash[term] = 1\n",
    "    return listHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workfolder = os.environ.get('AZUREML_NATIVE_SHARE_DIRECTORY')\n",
    "\n",
    "# paths to trainQ, testQ and function words.\n",
    "trainQ_path = os.path.join(workfolder, 'trainQ_part1')\n",
    "testQ_path = os.path.join(workfolder, 'testQ_part1')\n",
    "function_words_url = 'https://bostondata.blob.core.windows.net/stackoverflow/function_words.txt'\n",
    "\n",
    "# load the training and test data.\n",
    "trainQ = pd.read_csv(trainQ_path, sep='\\t', index_col='Id', encoding='latin1')\n",
    "testQ = pd.read_csv(testQ_path, sep='\\t', index_col='Id', encoding='latin1')\n",
    "\n",
    "# Load the list of non-content bearing function words.\n",
    "functionwordHash = LoadListAsHash(function_words_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Split the Text\n",
    "\n",
    "The CleanAndSplitText function from __phrase_learning__ takes as input a list where each row element is a single cohesive long string of text, i.e. a \"question\". The function first splits each string by various forms of punctuation into chunks of text that are likely sentences, phrases or sub-phrases. The splitting is designed to prohibit the phrase learning process from using cross-sentence or cross-phrase word strings when learning phrases.\n",
    "\n",
    "The function returns a table where each row represents a chunk of text from the questions. The `DocID` coulmn indicates the original row index from associated question in the input from which the chunk of text originated. The `DocLine` column contains the original text excluding the punctuation marks and `HTML` markup that have been during the cleaning process. The `Lowercase Taxt` column contains a fully lower-cased version of the text in the `CleanedText` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "CleanedTrainQ = CleanAndSplitText(trainQ)\n",
    "CleanedTestQ = CleanAndSplitText(testQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>DocLine</th>\n",
       "      <th>CleanedText</th>\n",
       "      <th>LowercaseText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69913</td>\n",
       "      <td>0</td>\n",
       "      <td>why don't self-closing script tags work</td>\n",
       "      <td>why don't self-closing script tags work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69913</td>\n",
       "      <td>1</td>\n",
       "      <td>what is the reason browsers do not correctly r...</td>\n",
       "      <td>what is the reason browsers do not correctly r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69913</td>\n",
       "      <td>2</td>\n",
       "      <td>only this is recognized</td>\n",
       "      <td>only this is recognized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69913</td>\n",
       "      <td>3</td>\n",
       "      <td>does this break the concept of xhtml support</td>\n",
       "      <td>does this break the concept of xhtml support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69913</td>\n",
       "      <td>4</td>\n",
       "      <td>note</td>\n",
       "      <td>note</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DocID  DocLine                                        CleanedText  \\\n",
       "0  69913        0            why don't self-closing script tags work   \n",
       "1  69913        1  what is the reason browsers do not correctly r...   \n",
       "2  69913        2                            only this is recognized   \n",
       "3  69913        3       does this break the concept of xhtml support   \n",
       "4  69913        4                                               note   \n",
       "\n",
       "                                       LowercaseText  \n",
       "0            why don't self-closing script tags work  \n",
       "1  what is the reason browsers do not correctly r...  \n",
       "2                            only this is recognized  \n",
       "3       does this break the concept of xhtml support  \n",
       "4                                               note  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CleanedTrainQ.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Informative Phrases \n",
    "The phrases can be treated as single compound word units in down-stream processes such as discriminative training. To learn the phrases, we have implemented the basic framework that combines key phrase learning and latent topic modeling as described in the paper entitled [\"Modeling Multiword Phrases with Constrained Phrases Tree for Improved Topic Modeling of Conversational Speech\"](http://people.csail.mit.edu/hazen/publications/Hazen-SLT-2012.pdf) which was originally presented in the 2012 IEEE Workshop on Spoken Language Technology. Although the paper examines the use of the technology for analyzing human-to-human conversations, the techniques are quite general and can be applied to a wide range of natural language data including news stories, legal documents, research publications, social media forum discussions, customer feedback forms, product reviews, and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start phrase learning with 0 phrases of 200 phrases learned\n",
      "Iteration 1: Added 42 new phrases in 1.15 seconds (Learned 42 of max 200)\n",
      "Iteration 2: Added 35 new phrases in 1.44 seconds (Learned 77 of max 200)\n",
      "Iteration 3: Added 32 new phrases in 1.36 seconds (Learned 109 of max 200)\n",
      "Iteration 4: Added 34 new phrases in 1.23 seconds (Learned 143 of max 200)\n",
      "Iteration 5: Added 31 new phrases in 1.23 seconds (Learned 174 of max 200)\n",
      "Iteration 6: Added 11 new phrases in 1.12 seconds (Learned 185 of max 200)\n",
      "Iteration 7: Added 3 new phrases in 1.14 seconds (Learned 188 of max 200)\n",
      "Iteration 8: Added 4 new phrases in 1.20 seconds (Learned 192 of max 200)\n",
      "Iteration 9: Added 1 new phrases in 1.21 seconds (Learned 193 of max 200)\n",
      "Iteration 10: Added 1 new phrases in 1.09 seconds (Learned 194 of max 200)\n",
      "Iteration 11: Added 1 new phrases in 1.25 seconds (Learned 195 of max 200)\n",
      "Iteration 12: Added 1 new phrases in 1.17 seconds (Learned 196 of max 200)\n",
      "Iteration 13: Added 1 new phrases in 1.10 seconds (Learned 197 of max 200)\n",
      "Iteration 14: Added 1 new phrases in 1.18 seconds (Learned 198 of max 200)\n",
      "Iteration 15: Added 1 new phrases in 1.20 seconds (Learned 199 of max 200)\n",
      "Iteration 16: Added 1 new phrases in 1.09 seconds (Learned 200 of max 200)\n",
      "*** Phrase learning completed in 0.01 hours ***\n",
      "Counting words\n",
      "Building vocab\n",
      "Excluded 307 stop words\n",
      "Excluded 911 non-alphabetic words\n",
      "Excluded 15266 words below word count threshold\n",
      "Excluded 142 words below doc count threshold\n",
      "Excluded 3 words above max doc frequency\n",
      "Final Vocab Size: 3115 words\n"
     ]
    }
   ],
   "source": [
    "# Create a structure defining the settings and word lists used during the phrase learning\n",
    "learningSettings = namedtuple('learningSettings',['maxNumPhrases','maxPhrasesPerIter',\n",
    "                                                'maxPhraseLength','minInstanceCount'\n",
    "                                                'functionwordHash','blacklistHash','verbose'])\n",
    "\n",
    "# If true it prints out the learned phrases to stdout buffer\n",
    "# while its learning. This will generate a lot of text to stdout, \n",
    "# so best to turn this off except for testing and debugging\n",
    "learningSettings.verbose = False\n",
    "\n",
    "# Maximium number of phrases to learn\n",
    "# If you want to test the code out quickly then set this to a small\n",
    "# value (e.g. 100) and set verbose to true when running the quick test\n",
    "learningSettings.maxNumPhrases = 200\n",
    "\n",
    "# Maximum number of phrases to learn per iteration \n",
    "# Increasing this number may speed up processing but will affect the ordering of the phrases \n",
    "# learned and good phrases could be by-passed if the maxNumPhrases is set to a small number\n",
    "learningSettings.maxPhrasesPerIter = 50\n",
    "\n",
    "# Maximum number of words allowed in the learned phrases \n",
    "learningSettings.maxPhraseLength = 7\n",
    "\n",
    "# Minimum number of times a phrase must occur in the data to \n",
    "# be considered during the phrase learning process\n",
    "learningSettings.minInstanceCount = 5\n",
    "\n",
    "# This is a precreated hash table containing the list \n",
    "# of function words used during phrase learning\n",
    "learningSettings.functionwordHash = functionwordHash\n",
    "\n",
    "# This is a precreated hash table containing the list \n",
    "# of black list words to be ignored during phrase learning\n",
    "learningSettings.blacklistHash = {}\n",
    "\n",
    "# Initialize an empty list of learned phrases\n",
    "# If you have completed a partial run of phrase learning\n",
    "# and want to add more phrases, you can use the pre-learned \n",
    "# phrases as a starting point instead and the new phrases\n",
    "# will be appended to the list\n",
    "learnedPhrasesQ = []\n",
    "\n",
    "# Create a copy of the original text data that will be used during learning\n",
    "# The copy is needed because the algorithm does in-place replacement of learned\n",
    "# phrases directly on the text data structure it is provided\n",
    "phraseTextDataQ = []\n",
    "for textLine in CleanedTrainQ['LowercaseText']:\n",
    "    phraseTextDataQ.append(' ' + textLine + ' ')\n",
    "\n",
    "# Run the phrase learning algorithm\n",
    "ApplyPhraseLearning(phraseTextDataQ, learnedPhrasesQ, learningSettings)\n",
    "\n",
    "# Add text with learned phrases back into data frame\n",
    "CleanedTrainQ['TextWithPhrases'] = phraseTextDataQ\n",
    "\n",
    "# apply the phrase learning to test data.\n",
    "CleanedTestQ['TextWithPhrases'] = ApplyPhraseRewritesInPlace(CleanedTestQ, 'LowercaseText', learnedPhrasesQ)\n",
    "\n",
    "# reconstitue the text from seperated chunks.\n",
    "trainQ['TextWithPhrases'] = ReconstituteDocsFromChunks(CleanedTrainQ, 'DocID', 'TextWithPhrases')\n",
    "testQ['TextWithPhrases'] = ReconstituteDocsFromChunks(CleanedTestQ, 'DocID', 'TextWithPhrases')\n",
    "\n",
    "# create the vocabulary.\n",
    "vocabHashQ = CreateVocabForTopicModeling(trainQ['TextWithPhrases'],functionwordHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are some phrases we learned in this part of the tutorial: \n",
      "\n",
      "['possible duplicate', \"i'm trying\", 'works fine', 'doing wrong', 'click event', 'following code', 'using jquery', 'uncaught typeerror', 'ajax request', 'global variable', 'div class', 'json object', 'callback function', \"i'm not sure\", 'anonymous function', 'php file', 'return value', 'user clicks', 'dynamically created', 'input type', 'javascript function', 'php variable', 'greatly appreciated', \"i'm having\", 'array of objects', 'best way', 'javascript variable', 'google maps', 'read property', 'present on the requested', 'json file', 'chrome extension', 'true or false', 'json data', 'java script', 'better way', 'div id', 'help would be appreciated', 'console log', 'single quotes', 'query string', 'return the response', \"i'm using\", 'event handler', 'header is present_on_the_requested', 'php code', 'span class', 'works perfectly', 'js file', 'simple practical', 'returns undefined', 'ajax response', 'regular expression', 'cross domain', 'piece of code', 'dynamically generated', \"what's the difference\", 'help would be greatly_appreciated', 'return false', 'first time', 'dynamically added', 'html file', \"i'm getting\", \"what's wrong\", 'inside loops', 'xmlhttprequest cannot load', \"here's my code\", 'query_string values', 'php echo', 'make it work', 'var functionname', 'return true', 'double quotes', 'decimal places', 'html page', 'same-origin policy', 'error message', 'using javascript', 'closure inside_loops', 'onclick event', 'jquery ajax', 'header_is_present_on_the_requested resource', 'using ajax', 'simple_practical example', 'html code', 'web page', 'function is called', 'return a value', 'circumvent the same-origin_policy', 'return the value', 'button is clicked', 'script src', 'having trouble', 'img src', 'stack overflow', \"i'm doing_wrong\", 'working fine', 'pass a variable', 'make sure', 'right direction', 'access a specific', 'php page', 'want to add', 'doing something wrong', 'help is appreciated', 'json string', 'return_the_response from an ajax', 'dot notation', 'newly added', 'javascript code', 'javascript object', 'javascript file', 'event listener', 'php script', 'ways to circumvent_the_same-origin_policy', 'click the button', 'variable outside', 'dollar sign', \"i'm working\", 'google map', 'doesnt work', 'json response', 'solve this problem', 'trying to make', 'double or single_quotes', 'appreciate any help', \"i'm_having trouble\", \"col style'width\", 'difference between these two', 'submit button', 'doctype html&gt', 'tried using', 'page load', 'origin policy', 'allowed access', 'parseint octal', 'string to a javascript_variable', 'http request', 'json array', 'js code', \"here's the code\", 'way of doing', 'module pattern', 'new to javascript', 'object literal', 'script type', 'local variable', 'trying to access', 'correct value', 'alert box', \"i'm sure\", 'works just fine', 'web app', 'start with a dollar_sign', 'latitude and longitude', 'parseint_octal bug', 'file upload', 'array of javascript_objects', \"i'm writing\", 'tried the following', \"what's the best_way\", 'inside a for loop', 'ajax calls', 'makes sense', 'data structure', 'help will be appreciated', 'trying to create', 'want to change', 'best practice', 'web service', 'using var', 'lazy about evaluating', 'want to access', 'uncaught referenceerror', 'javascript closure_inside_loops', 'new tab', 'using_var and not using_var', 'nested data_structure', 'content script', 'xml file', 'work at all', 'floating point', 'self-references in object_literal', 'need to access', 'newly created', 'function in javascript', \"i'm new\", 'create a new', 'var_functionname function', 'click function', 'correct way', \"i'm looking\", 'var_functionname function', 'var_functionname function', 'var_functionname function', 'var_functionname function', 'var_functionname function', 'var_functionname function', 'var_functionname function', 'var_functionname function']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nThere are some phrases we learned in this part of the tutorial: \\n\")\n",
    "print(learnedPhrasesQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text with Learned Phrases\n",
    "\n",
    "Next, we break the reconstituted text into individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize the full text string for each question into list of tokens.\n",
    "# any token that is in not in the pre-defined set of acceptable vocabulary words is execluded.\n",
    "def TokenizeText(textData,vocabHash):\n",
    "    tokenizedText = ''\n",
    "    for token in textData.split():\n",
    "        if token in vocabHash:\n",
    "            tokenizedText += (token.strip() + ',')\n",
    "    return tokenizedText.strip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize the text.\n",
    "trainQ['Tokens'] = trainQ['TextWithPhrases'].apply(lambda x: TokenizeText(x, vocabHashQ))\n",
    "testQ['Tokens'] = testQ['TextWithPhrases'].apply(lambda x: TokenizeText(x, vocabHashQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnswerId</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69913</th>\n",
       "      <td>69984</td>\n",
       "      <td>self-closing,script,tags,work,reason,browsers,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392561</th>\n",
       "      <td>69984</td>\n",
       "      <td>firefox,script,tag,error,adding,basic,script,t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297308</th>\n",
       "      <td>69984</td>\n",
       "      <td>weird,javascript/jquery,behavior,possible_dupl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352182</th>\n",
       "      <td>69984</td>\n",
       "      <td>html,script,tags,ending,possible_duplicate,t,s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5355867</th>\n",
       "      <td>69984</td>\n",
       "      <td>loading,scripts,possible_duplicate,don&amp;#39,t,s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AnswerId                                             Tokens\n",
       "Id                                                                  \n",
       "69913       69984  self-closing,script,tags,work,reason,browsers,...\n",
       "392561      69984  firefox,script,tag,error,adding,basic,script,t...\n",
       "1297308     69984  weird,javascript/jquery,behavior,possible_dupl...\n",
       "3352182     69984  html,script,tags,ending,possible_duplicate,t,s...\n",
       "5355867     69984  loading,scripts,possible_duplicate,don&#39,t,s..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainQ[['AnswerId', 'Tokens']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Outputs to a Share Directory in the Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainQ.to_csv(os.path.join(workfolder, 'trainQ_part2'), sep='\\t', header=True, index=True, index_label='Id')\n",
    "testQ.to_csv(os.path.join(workfolder, 'testQ_part2'), sep='\\t', header=True, index=True, index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qna-matching local",
   "language": "python",
   "name": "qna-matching_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
